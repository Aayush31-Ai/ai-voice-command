"""
rag_service.py â€” PDF indexing and retrieval using Qdrant (local Docker) + Ollama embeddings.
Chat answers are generated by Groq.
"""

from __future__ import annotations
import logging
import os
import tempfile
from typing import List

logger = logging.getLogger(__name__)

CHUNK_SIZE = 800
CHUNK_OVERLAP = 150


# ---------------- CLIENTS ---------------- #

def _env(name: str, default: str = "") -> str:
    return os.getenv(name, default)


def _qdrant_url() -> str:
    configured = _env("QDRANT_URL", "").strip()
    if configured:
        return configured
    host = _env("QDRANT_HOST", "localhost")
    port = _env("QDRANT_PORT", "6333")
    return f"http://{host}:{port}"

def _make_client():
    from qdrant_client import QdrantClient

    return QdrantClient(
        url=_qdrant_url(),
        api_key=_env("QDRANT_API_KEY", "") or None,
        prefer_grpc=False,
    )


def _make_embeddings():
    from langchain_ollama import OllamaEmbeddings

    return OllamaEmbeddings(
        model=_env("OLLAMA_EMBED_MODEL", "nomic-embed-text-v2-moe"),
        base_url=_env("OLLAMA_BASE_URL", "http://localhost:11434"),
    )


def _make_llm():
    from langchain_groq import ChatGroq

    api_key = _env("GROQ_API_KEY", "").strip()
    if not api_key:
        raise RuntimeError("GROQ_API_KEY is not set in environment")

    return ChatGroq(
        model=_env("GROQ_CHAT_MODEL", "llama-3.3-70b-versatile"),
        api_key=api_key,
        temperature=0.2,
    )


# ---------------- COLLECTION SAFE CREATE ---------------- #

def _ensure_collection(collection_name: str, embedding_dim: int) -> None:
    """
    Create collection dynamically using real embedding dimension.
    If exists with wrong dimension â†’ delete & recreate.
    """
    from qdrant_client.http.models import Distance, VectorParams

    client = _make_client()

    try:
        collection = client.get_collection(collection_name)
        existing_dim = collection.config.params.vectors.size

        if existing_dim != embedding_dim:
            logger.warning(
                "Dimension mismatch (existing=%s, new=%s). Recreating collection...",
                existing_dim,
                embedding_dim,
            )
            client.delete_collection(collection_name)
            raise Exception("Recreate")

        logger.info("Collection already exists with correct dimension.")

    except Exception:
        client.recreate_collection(
            collection_name=collection_name,
            vectors_config=VectorParams(
                size=embedding_dim,
                distance=Distance.COSINE,
            ),
        )
        logger.info("Created collection '%s' (dim=%s)", collection_name, embedding_dim)


# ---------------- INDEX PDF ---------------- #

def index_pdf(pdf_bytes: bytes, collection_name: str) -> int:
    from langchain_community.document_loaders import PyPDFLoader
    from langchain_text_splitters import RecursiveCharacterTextSplitter
    from langchain_qdrant import Qdrant

    with tempfile.NamedTemporaryFile(suffix=".pdf", delete=False) as tmp:
        tmp.write(pdf_bytes)
        tmp_path = tmp.name

    try:
        loader = PyPDFLoader(tmp_path)
        docs = loader.load()
    finally:
        os.unlink(tmp_path)

    if not docs:
        return 0

    splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP,
    )

    chunks = splitter.split_documents(docs)

    embeddings = _make_embeddings()

    # ðŸ”¥ Dynamically detect embedding dimension
    test_vector = embeddings.embed_query("dimension test")
    embedding_dim = len(test_vector)

    _ensure_collection(collection_name, embedding_dim)

    client = _make_client()
    vector_store = Qdrant.from_documents(
        documents=chunks,
        embedding=embeddings,
        client=client,
        collection_name=collection_name,
    )

    return len(chunks)


# ---------------- SEARCH ---------------- #

def search_context(question: str, collection_name: str, k: int = 3):
    from langchain_qdrant import Qdrant

    embeddings = _make_embeddings()

    vector_store = Qdrant(
        client=_make_client(),
        collection_name=collection_name,
        embeddings=embeddings,
    )

    results = vector_store.similarity_search(question, k=k)

    if not results:
        return "", []

    context_parts = []
    sources = []

    for result in results:
        meta = result.metadata or {}
        page = meta.get("page", "?")
        source = meta.get("source", "PDF")

        context_parts.append(
            f"Page Content:\n{result.page_content}\nPage: {page}\nSource: {source}"
        )
        sources.append(f"Page {page} â€” {source}")

    return "\n\n---\n\n".join(context_parts), sources


# ---------------- ASK ---------------- #

def ask(question: str, collection_name: str, k: int = 3) -> dict:
    from langchain_core.messages import HumanMessage, SystemMessage

    context, sources = search_context(question, collection_name, k)

    if not context:
        return {
            "answer": "I couldn't find relevant information in the document.",
            "sources": [],
        }

    llm = _make_llm()

    messages = [
        SystemMessage(
            content=(
                "Answer strictly based on the provided context. "
                "If not found in context, say so.\n\n"
                f"Context:\n{context}"
            )
        ),
        HumanMessage(content=question),
    ]

    response = llm.invoke(messages)

    return {
        "answer": response.content,
        "sources": sources,
    }


# ---------------- DELETE ---------------- #

def delete_collection(collection_name: str) -> None:
    client = _make_client()
    client.delete_collection(collection_name)
